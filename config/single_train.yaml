# config.yaml
#
# ## Where the samples will be written

# ## Where the vocab(s) will be written
src_vocab: precursor_prediction/data/vocab.src
tgt_vocab: precursor_prediction/data/vocab.tgt
# # Prevent overwriting existing files in the folder
overwrite: False

share_vocab: False
# # Corpus opts:
data:
    train:
        path_src: precursor_prediction/data/train_src.txt
        path_tgt: precursor_prediction/data/train_tgt.txt
        
    valid:
        path_src: precursor_prediction/data/valid_src.txt
        path_tgt: precursor_prediction/data/valid_tgt.txt
#src_seq_length: 300
#tgt_seq_length: 300


# General opts
save_model: precursor_prediction/checkpoints/retrosyn
save_checkpoint_steps: 10000
keep_checkpoint: 30
train_steps: 300000
seed: 1234

# Batching
gpu_ranks: [0]
max_generator_batches: 32
batch_type: "tokens"
batch_size: 4096
accum_count: [4]
report_every: 2000
#world_size: 2

# # Optimization
optim: "adam"
learning_rate: 2
warmup_steps: 8000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 0
param_init: 0
param_init_glorot: true
normalization: "tokens"
label_smoothing: 0.0

# # Model
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 4
dec_layers: 4
heads: 8
rnn_size: 256
word_vec_size: 256
transformer_ff: 2048
dropout: [0.1]
share_embeddings: False
global_attention: general
global_attention_function: softmax
self_attn_type: scaled-dot
